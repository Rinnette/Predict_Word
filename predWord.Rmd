---
title: "Word Prediction using SwiftKey data"
author: "Rinnette N. Ramdhanie"
date: "20 March 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadlibs}
        library(tm)
        library(SnowballC)
        library(ggplot2)
        library(RWeka)

```

## Read Data
```{r readFiles}

        con <- file("../data/en_US.twitter.txt", "r")
        twitter <- readLines(con)
        close(con)

        con <- file("../data/en_US.blogs.txt", "r")
        blogs <- readLines(con)
        close(con)
        
        con <- file("../data/en_US.news.txt", "rb")
        news <- readLines(con)
        close(con)
```

```{r summaryStats}
        numWords_twitter <- sum(str_count(twitter, '\\w+'))
        numWords_blogs <- sum(str_count(blogs, '\\w+'))
        numWords_news <- sum(str_count(news, '\\w+'))

        summaryStats <- data.frame("FileName" = c("Twitter", "Blogs", "News"),
                                   "NumberOfLines" = c(length(twitter), length(blogs), length(news)),
                                   "NumberOfWords" = c(numWords_twitter, numWords_blogs, 
                                                       numWords_news))
        summaryStats

```


```{r getSamples}

        sampleTwitter <- twitter[rbinom(n = length(twitter), size = 1, prob = 0.1) == 1]
        sampleBlogs <- blogs[rbinom(n = length(blogs), size = 1, prob = 0.1) == 1]
        sampleNews <- news[rbinom(n = length(news), size = 1, prob = 0.1) == 1]
        
        sampleAll <- c(sampleTwitter, sampleBlogs, sampleNews)
        
        outCon <- file("../data/sample.txt", "w")
        writeLines(sampleAll, outCon)
        close(outCon)
        
        txtCorpus <- VCorpus(VectorSource(sampleAll))

```

## Clean data
```{r cleanData}
       
        cleanCorpus <- tm_map(txtCorpus, stripWhitespace)
        cleanCorpus <- tm_map(cleanCorpus, content_transformer(tolower))
        cleanCorpus <- tm_map(cleanCorpus, removePunctuation)
        cleanCorpus <- tm_map(cleanCorpus, removeNumbers)
        cleanCorpus <- tm_map(cleanCorpus, removeWords, stopwords("english"))
        cleanCorpus <- tm_map(cleanCorpus, stemDocument)
        #cleanCorpus <- tm_map(cleanCorpus, PlainTextDocument)

```

## Exploratory Analysis

```{r createDTM}
        #dtm <- DocumentTermMatrix(cleanCorpus)
                tdm <- TermDocumentMatrix((cleanCorpus))

```

```{r findFreqWords}
        # Find terms that occur at least 1000 times
                #freqTerms <- dtm[ ,findFreqTerms(dtm, 1000)]
                       freqTerms <- tdm[findFreqTerms(dtm, 1000), ] 
        
        # Find the number of times each word occurs across all documents
                #freqSum <- colSums(as.matrix(freqTerms))
                        freqSum <- rowSums(as.matrix(freqTerms))
                
        # Order frequency from highest occuring to lowest
                freqOrdered <- freqSum[order(freqSum, decreasing = TRUE)]
                
        # Save ordered frequencies to a data frame
                wordFreqDF <- data.frame(word = names(freqOrdered), freq = freqOrdered)

```

```{r plotFreqWords}

        # plot graph of the top 20 words
                top20Words <- head(wordFreqDF, 20)
                ggplot(top20, aes(x = reorder(word, -freq), y=freq)) +
                        geom_bar(stat = "identity", fill = "#3296FF") +
                        ggtitle("Top 20 Words") +
                        scale_y_continuous(name = "Frequency", breaks=seq(0, 35000, 5000)) +
                        xlab("Most Frequently Occurring Words") +
                        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
                        coord_flip()

```


```{r getBigrams}
        BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "),
                                              use.names = FALSE)
        bigramTDM = TermDocumentMatrix(cleanCorpus, control = list(tokenize = BigramTokenizer))
        # control = list(bounds = list(local = c(2,Inf)))

        freqBG <- rowSums(as.matrix(bigramTDM[findFreqTerms(bigramTDM, 100), ]))
        BGOrdered <- freqBG[order(freqBG, decreasing = TRUE)]
        
        bigramDF <- data.frame(word = names(BGOrdered), freq = BGOrdered)
        
```

```{r plotFreqBigrams}

        # plot graph of the top 20 bigrams
                top20BG <- head(bigramDF, 20)
                ggplot(top20BG, aes(x = reorder(word, -freq), y=freq)) +
                        geom_bar(stat = "identity", fill = "#008040") +
                        ggtitle("Top 20 Bigrams") +
                        scale_y_continuous(name = "Frequency", breaks=seq(0, 2500, 250)) +
                        xlab("Most Frequently Occurring Bigrams") +
                        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
                        coord_flip()

```

```{r getTrigrams}
        TrigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "),
                                              use.names = FALSE)
        trigramTDM = TermDocumentMatrix(cleanCorpus, control = list(tokenize = TrigramTokenizer))
        # control = list(bounds = list(local = c(2,Inf)))

        freqTG <- rowSums(as.matrix(trigramTDM[findFreqTerms(trigramTDM, 100), ]))
        TGOrdered <- freqTG[order(freqTG, decreasing = TRUE)]
        
        trigramDF <- data.frame(word = names(TGOrdered), freq = TGOrdered)
        
```

```{r plotFreqTrigrams}

        # plot graph of the top 20 bigrams
                top20TG <- head(trigramDF, 20)
                ggplot(top20TG, aes(x = reorder(word, -freq), y=freq)) +
                        geom_bar(stat = "identity", fill = "#0a234a") +
                        ggtitle("Top 20 Trigrams") +
                        scale_y_continuous(name = "Frequency", breaks=seq(0, 350, 50)) +
                        xlab("Most Frequently Occurring Trigrams") +
                        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
                        coord_flip()

```